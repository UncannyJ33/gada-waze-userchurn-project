{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtNBZFHO3M7n"
   },
   "source": [
    "# **Waze User Churn: Tree‑Based Machine Learning**\n",
    "\n",
    "This notebook builds and evaluates two tree‑based models—Random Forest and XGBoost—to predict Waze user churn based on behavioral and usage features. The focus is on model performance (with recall as the primary metric), feature importance, and how these models complement the earlier logistic regression baseline.\n",
    "\n",
    "The workflow includes:\n",
    "- Feature engineering and encoding for tree‑based models  \n",
    "- Model training with cross‑validated hyperparameter tuning  \n",
    "- Model selection using a validation set and final evaluation on a held‑out test set  \n",
    "- Interpretation of model performance and feature importance in a business context  \n",
    "\n",
    "### **Tree‑based modeling overview**\n",
    "\n",
    "Decision tree–based models split the feature space by asking a sequence of if/else questions on features (for example, “is `drives` > threshold?”) and assign churn probabilities to each resulting region. Random Forest builds many such trees on bootstrapped samples and averages their predictions to reduce variance, while XGBoost builds trees sequentially to correct previous errors, often achieving stronger performance at the cost of greater complexity and interpretability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8Vm3QEfGELS"
   },
   "source": [
    "## **Imports and data loading**\n",
    "\n",
    "Core libraries for data manipulation, visualization, and model training are imported, and the Waze churn dataset is loaded into a working DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fKhnX2Puf4Bt"
   },
   "outputs": [],
   "source": [
    "# Import packages for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import packages for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Display all columns in DataFrame outputs\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Import packages for model selection and evaluation\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,\\\n",
    "f1_score, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n",
    "\n",
    "# Tree-based classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "\n",
    "# Model persistence\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5weTXGKqa_iG"
   },
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "df0 = pd.read_csv('waze_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1HyORSaQo_LU"
   },
   "outputs": [],
   "source": [
    "# Inspect the first five rows\n",
    "df0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw Waze dataset is first loaded into `df0` for inspection, then copied into `df` for feature engineering so the original data remains unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VZowX9rhU1o"
   },
   "source": [
    "## **Feature engineering**\n",
    "\n",
    "Previous EDA and statistical modeling identified usage‑based predictors and highlighted derived features related to intensity and recency. This section engineers those features for tree‑based models, then handles missing labels and encodes categorical variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBOSW8IDbO_d"
   },
   "outputs": [],
   "source": [
    "# Work on a copy of the original data\n",
    "df = df0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "teUeCF-yf_6o"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPtJEHjcuepR"
   },
   "source": [
    "#### **`km_per_driving_day`**\n",
    "\n",
    "Average kilometers driven per driving day captures driving intensity for users who drove at least once in the month and helps distinguish casual drivers from very heavy users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vAB6cv6xfvZn"
   },
   "outputs": [],
   "source": [
    "# Mean kilometers per driving day\n",
    "df['km_per_driving_day'] = df['driven_km_drives']/df['driving_days']\n",
    "\n",
    "# Inspect distribution\n",
    "df['km_per_driving_day'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vv3owriWuuDQ"
   },
   "outputs": [],
   "source": [
    "# replace infinite values (from 0 driving days) with 0 to keep statistics valid\n",
    "df.loc[df['km_per_driving_day']==np.inf, 'km_per_driving_day']=0\n",
    "df['km_per_driving_day'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZfNE37b-LlJ"
   },
   "source": [
    "#### **`percent_sessions_in_last_month`**\n",
    "`percent_sessions_in_last_month` represents the fraction of a user’s lifetime sessions that occurred in the last observed month, highlighting recent engagement intensity and behavior changes leading up to churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mRefXCF-K_c"
   },
   "outputs": [],
   "source": [
    "# Share of lifetime sessions that occurred in the last month\n",
    "df['percent_sessions_in_last_month'] = df['sessions']/df['total_sessions']\n",
    "\n",
    "# Inspect distribution\n",
    "df['percent_sessions_in_last_month'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjgkLrOf_OrE"
   },
   "source": [
    "#### **`professional_driver`**\n",
    "A binary `professional_driver` flag is used to approximate heavy, likely professional users (≥60 drives and ≥15 driving days in the last month), based on domain‑informed thresholds and earlier EDA showing lower churn among high‑activity users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQdMgikKU-5T"
   },
   "outputs": [],
   "source": [
    "# Flag high‑intensity \"professional\" drivers\n",
    "df['professional_driver'] = np.where((df['drives']>=60) & (df['driving_days']>=15), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3bWzofHVIuC"
   },
   "source": [
    "#### **`total_sesions_per_day`**\n",
    "`total_sesions_per_day` (as named in the data) estimates the average number of sessions per day since onboarding, capturing long‑term engagement intensity over a user’s lifetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWXPMPHSVJQd"
   },
   "outputs": [],
   "source": [
    "# Average sessions per day since onboarding\n",
    "df['total_sesions_per_day'] = df['total_sessions']/df['n_days_after_onboarding']\n",
    "df['total_sesions_per_day'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6sCAgOoVZM7"
   },
   "source": [
    "#### **`km_per_hour`**\n",
    "`km_per_hour` summarizes average driving speed over the month, combining distance and time into a single efficiency metric that may reflect different driving contexts (for example, highway vs. urban driving)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zu142H3aVc3o"
   },
   "outputs": [],
   "source": [
    "# Average kilometers per hour across all drives in the last month\n",
    "df['km_per_hour'] = df['driven_km_drives']/(df['duration_minutes_drives']/60)\n",
    "df['km_per_hour'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d6N9jf8ViW-"
   },
   "source": [
    "#### **`km_per_drive`**\n",
    "`km_per_drive` represents average distance per trip, providing another view on driving intensity per drive. As with other ratio features, division by zero for users with no drives produces infinite values, which are recoded to 0 for stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5R5-MteVlMB"
   },
   "outputs": [],
   "source": [
    "# Average kilometers per drive in the last month\n",
    "df['km_per_drive'] = df['driven_km_drives']/df['drives']\n",
    "df['km_per_drive'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZrHMuPuVmIt"
   },
   "outputs": [],
   "source": [
    "# Replace infinite values (from 0 drives) with 0\n",
    "df.loc[df['km_per_drive']==np.inf, 'km_per_drive'] = 0\n",
    "df['km_per_drive'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5Sxs6agVunA"
   },
   "source": [
    "#### **`percent_of_sessions_to_favorite`**\n",
    "`percent_of_sessions_to_favorite` approximates the share of sessions spent navigating to saved favorite locations, serving as a proxy for how often users travel to familiar vs. new places, which may relate to exploration and dependence on navigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vh22o46AVxd_"
   },
   "outputs": [],
   "source": [
    "# Share of sessions used to navigate to favorite places\n",
    "df['percent_of_sessions_to_favorite'] = (df['total_navigations_fav1']+df['total_navigations_fav2'])/df['total_sessions']\n",
    "\n",
    "# Inspect distribution\n",
    "df['percent_of_sessions_to_favorite'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZO0mvHRWGmF"
   },
   "source": [
    "### **Handle missing churn labels**\n",
    "\n",
    "As in earlier notebooks, rows with missing churn labels (`label`) are dropped because they represent <5% of observations and show no clear non‑random pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TdA6SnGWJY-"
   },
   "outputs": [],
   "source": [
    "# Drop rows with missing churn labels\n",
    "df = df.dropna(subset=['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Du5kGt5CWJ4J"
   },
   "source": [
    "### **Outliers and tree‑based models**\n",
    "\n",
    "Many usage variables contain outliers, but tree‑based models are generally robust to extreme values, so no additional outlier imputation is applied for this stage of modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxBYyXDSWPkw"
   },
   "source": [
    "### **Variable encoding**\n",
    "\n",
    "#### **Device**\n",
    "\n",
    "The `device` feature (Android vs. iPhone) is encoded as a binary numeric predictor `device2` to be usable in scikit‑learn models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fntUcR4-aUfH"
   },
   "outputs": [],
   "source": [
    "# Binary‑encode device: Android = 0, iPhone = 1\n",
    "df['device2'] = np.where(df['device']=='Android',0,1)\n",
    "df[['device','device2']].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgbEm7cOb6t8"
   },
   "source": [
    "#### **Target variable**\n",
    "\n",
    "The churn label is encoded as `label2`, where 0 represents retained users and 1 represents churned users, preserving the original string labels for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jiVjplLb8W-"
   },
   "outputs": [],
   "source": [
    "# Create binary `label2` column\n",
    "df['label2'] = np.where(df['label']=='retained',0,1)\n",
    "df[['label','label2']].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fD_zG59eaV2c"
   },
   "source": [
    "### **Feature set definition**\n",
    "\n",
    "Tree‑based models can handle multicollinearity, so only the non‑informative identifier `ID` is dropped. The original `device` column is also excluded later in favor of the encoded `device2` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kf3uGtUQaWSL"
   },
   "outputs": [],
   "source": [
    "# Drop non‑informative ID column\n",
    "df = df.drop(['ID'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajj50RCCaXrF"
   },
   "source": [
    "### **Evaluation metric and class balance**\n",
    "\n",
    "The churn label is moderately imbalanced: about 18% of users churn and 82% are retained, which is manageable without explicit resampling. Because the main business risk is missing at‑risk users rather than incorrectly flagging some retained users, recall is used as the primary evaluation metric for model selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JkjEYByaYbr"
   },
   "outputs": [],
   "source": [
    "# Get class balance of 'label' column\n",
    "df['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5jzGjOS8iiv"
   },
   "source": [
    "## **Modeling workflow**\n",
    "\n",
    "The final modeling dataset contains 14,299 samples, enough to support a standard train/validation/test workflow for model training and selection. The process is:\n",
    "\n",
    "1. Split the data into train/validation/test sets (60/20/20).  \n",
    "2. Fit models and tune hyperparameters on the training set using cross‑validation.  \n",
    "3. Select a champion model based on validation recall.  \n",
    "4. Evaluate the champion model on the held‑out test set to estimate performance on new data.  \n",
    "\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/adacert/tiktok/main/optimal_model_flow_numbered.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nx41bVxX89Fe"
   },
   "source": [
    "### **Train/validation/test split**\n",
    "\n",
    "Features and target are defined and then split into 60/20/20 train, validation, and test sets using stratified sampling to preserve class balance in each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLbapbSWDUL-"
   },
   "outputs": [],
   "source": [
    "# Feature matrix (exclude label columns and original device)\n",
    "X = df.drop(columns=['label', 'label2','device'])\n",
    "\n",
    "# Binary target vector\n",
    "y = df['label2']\n",
    "\n",
    "# Initial 80/20 split into interim train and test sets\n",
    "X_tr, X_test, y_tr, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split interim train into final train (60%) and validation (20%) sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tr,y_tr, stratify=y_tr, test_size=0.25, random_state=42)\n",
    "\n",
    "# Verify partition sizes\n",
    "for x in [X_train, X_val, X_test]:\n",
    "    print(len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vynZs5het1b_"
   },
   "source": [
    "### **Random forest**\n",
    "\n",
    "A Random Forest classifier is tuned with a small hyperparameter grid using cross‑validated GridSearchCV, optimizing for recall while also tracking precision, F1, and accuracy. Random Forest builds an ensemble of decision trees on bootstrapped samples and averages their predictions, which reduces variance and typically improves generalization compared with a single decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vj5rJWOv5O3d"
   },
   "outputs": [],
   "source": [
    "# Instantiate the random forest classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameters to evaluate in cross‑validation\n",
    "cv_params = {'max_depth':[None],\n",
    "            'max_features':[1.0],\n",
    "            'max_samples':[1.0],\n",
    "            'min_samples_leaf': [2],\n",
    "            'min_samples_split': [2],\n",
    "            'n_estimators':[300]}\n",
    "\n",
    "# Metrics to record during grid search\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "# Cross‑validated grid search, refitting on the model with best recall\n",
    "rf_cv = GridSearchCV(rf, cv_params, scoring=scoring, cv=5, refit='recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is fit on the training data and the best cross‑validated recall score and hyperparameters are inspected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXuBiTGi5ZHn"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rf_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YtAgrH0zy4CE"
   },
   "outputs": [],
   "source": [
    "# Examine best score\n",
    "rf_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kazNtYG4fQOI"
   },
   "outputs": [],
   "source": [
    "# Examine best hyperparameter combo\n",
    "rf_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZZnem5yiAau"
   },
   "source": [
    "The `make_results()` helper function summarizes cross‑validated accuracy, precision, recall, and F1 for the best model according to a chosen metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-UodWEOedxz"
   },
   "outputs": [],
   "source": [
    "def make_results(model_name:str, model_object, metric:str):\n",
    "    '''\n",
    "    Arguments:\n",
    "        model_name (string): what you want the model to be called in the output table\n",
    "        model_object: a fit GridSearchCV object\n",
    "        metric (string): precision, recall, f1, or accuracy\n",
    "\n",
    "    Returns a pandas df with the F1, recall, precision, and accuracy scores\n",
    "    for the model with the best mean 'metric' score across all validation folds.\n",
    "    '''\n",
    "\n",
    "    # Create dictionary that maps input metric to actual metric name in GridSearchCV\n",
    "    metric_dict = {'precision': 'mean_test_precision',\n",
    "                   'recall': 'mean_test_recall',\n",
    "                   'f1': 'mean_test_f1',\n",
    "                   'accuracy': 'mean_test_accuracy',\n",
    "                   }\n",
    "\n",
    "    # Get all the results from the CV and put them in a df\n",
    "    cv_results = pd.DataFrame(model_object.cv_results_)\n",
    "\n",
    "    # Isolate the row of the df with the max(metric) score\n",
    "    best_estimator_results = cv_results.iloc[cv_results[metric_dict[metric]].idxmax(), :]\n",
    "\n",
    "    # Extract accuracy, precision, recall, and f1 score from that row\n",
    "    f1 = best_estimator_results.mean_test_f1\n",
    "    recall = best_estimator_results.mean_test_recall\n",
    "    precision = best_estimator_results.mean_test_precision\n",
    "    accuracy = best_estimator_results.mean_test_accuracy\n",
    "\n",
    "    # Create table of results\n",
    "    table = pd.DataFrame({'model': [model_name],\n",
    "                          'precision': [precision],\n",
    "                          'recall': [recall],\n",
    "                          'F1': [f1],\n",
    "                          'accuracy': [accuracy],\n",
    "                          },\n",
    "                         )\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diQezudIfzHn"
   },
   "source": [
    "The `make_results()` helper function summarizes cross‑validated accuracy, precision, recall, and F1 for the best model according to a chosen metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAYb2QigiT_h"
   },
   "outputs": [],
   "source": [
    "results = make_results('RF cv', rf_cv, 'recall')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SB-yhW9uu7dO"
   },
   "source": [
    "Aside from accuracy, the recall and F1 scores are modest, but recall is already markedly higher than in the earlier logistic regression baseline while maintaining similar accuracy. Additional hyperparameter tuning could yield small gains, but the current configuration provides a reasonable benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOlktJ6l4Tgt"
   },
   "source": [
    "### **XGBoost**\n",
    "\n",
    "An XGBoost classifier is then tuned over a slightly richer hyperparameter grid, again optimizing for recall while tracking other metrics. XGBoost builds trees sequentially, where each new tree is trained to correct the residual errors of the previous ensemble, enabling the model to capture complex nonlinear relationships and interactions in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ciO48nhiTqO"
   },
   "outputs": [],
   "source": [
    "# Instantiate the XGBoost classifier for binary classification\n",
    "xgb = XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# Hyperparameters to evaluate in cross‑validation\n",
    "cv_params = {'max_depth': [6,12],\n",
    "            'min_child_weight': [3,5],\n",
    "            'learning_rate': [0.01,0.1],\n",
    "            'n_estimators': [300]}\n",
    "\n",
    "# Metrics to record during grid search\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "# Cross‑validated grid search, refitting on the model with best recall\n",
    "xgb_cv = GridSearchCV(xgb, cv_params, scoring=scoring, cv=5, refit='recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYCWs_HX6804"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "xgb_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFLTmIDm6805"
   },
   "outputs": [],
   "source": [
    "# Examine best score\n",
    "xgb_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdPUCuND6805"
   },
   "outputs": [],
   "source": [
    "# Examine best parameters\n",
    "xgb_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QL19dH2h7KdD"
   },
   "outputs": [],
   "source": [
    "# Call 'make_results()' on the GridSearch object\n",
    "xgb_cv_results = make_results('XGB cv', xgb_cv, 'recall')\n",
    "results = pd.concat([results, xgb_cv_results], axis=0)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5IRnMO27KdD"
   },
   "source": [
    "The tuned XGBoost model achieves higher recall than both the logistic regression baseline and the random forest, while maintaining similar accuracy and precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfX0SjJffkh1"
   },
   "source": [
    "## **Model selection on validation data**\n",
    "\n",
    "The best random forest and XGBoost models from cross‑validation are evaluated on the validation set, and the model with higher recall is selected as the champion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chgR3Tx8fn1s"
   },
   "source": [
    "#### **Random forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUswawM2fyAf"
   },
   "outputs": [],
   "source": [
    "# Use random forest model to predict on validation data\n",
    "rf_val_preds = rf_cv.best_estimator_.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJ9mCl0Uf4P4"
   },
   "outputs": [],
   "source": [
    "def get_test_scores(model_name:str, preds, y_test_data):\n",
    "    '''\n",
    "    Generate a table of test scores.\n",
    "\n",
    "    In:\n",
    "        model_name (string): Your choice: how the model will be named in the output table\n",
    "        preds: numpy array of test predictions\n",
    "        y_test_data: numpy array of y_test data\n",
    "\n",
    "    Out:\n",
    "        table: a pandas df of precision, recall, f1, and accuracy scores for your model\n",
    "    '''\n",
    "    accuracy = accuracy_score(y_test_data, preds)\n",
    "    precision = precision_score(y_test_data, preds)\n",
    "    recall = recall_score(y_test_data, preds)\n",
    "    f1 = f1_score(y_test_data, preds)\n",
    "\n",
    "    table = pd.DataFrame({'model': [model_name],\n",
    "                          'precision': [precision],\n",
    "                          'recall': [recall],\n",
    "                          'F1': [f1],\n",
    "                          'accuracy': [accuracy]\n",
    "                          })\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22ANR4ZHf5NK"
   },
   "outputs": [],
   "source": [
    "# Get validation scores for RF model\n",
    "rf_val_scores = get_test_scores('RF val', rf_val_preds, y_val)\n",
    "\n",
    "# Append to the results table\n",
    "results = pd.concat([results, rf_val_scores], axis=0)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDeuk16igBD0"
   },
   "source": [
    "Validation scores for the random forest drop slightly relative to cross‑validation results across all metrics, which is expected and suggests limited overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8h2s5RpgEER"
   },
   "source": [
    "#### **XGBoost**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQoTuRkngHjp"
   },
   "outputs": [],
   "source": [
    "# Use XGBoost model to predict on validation data\n",
    "xgb_val_preds = xgb_cv.best_estimator_.predict(X_val)\n",
    "\n",
    "# Get validation scores for XGBoost model\n",
    "xgb_val_scores = get_test_scores('XGB val', xgb_val_preds, y_val)\n",
    "\n",
    "# Append to the results table\n",
    "results = pd.concat([results, xgb_val_scores], axis=0)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GspkQqUNgIm3"
   },
   "source": [
    "The XGBoost model shows a similar small drop from cross‑validation to validation scores but still outperforms the random forest on recall, confirming it as the champion model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOm4n_1OgUND"
   },
   "source": [
    "## **Champion model evaluation on test data**\n",
    "\n",
    "The champion XGBoost model is applied to the held‑out test set to estimate performance on new, unseen users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7BkheTIsgU2b"
   },
   "outputs": [],
   "source": [
    "# Use XGBoost model to predict on test data\n",
    "xgb_test_preds = xgb_cv.best_estimator_.predict(X_test)\n",
    "\n",
    "# Get test scores for XGBoost model\n",
    "xgb_test_scores = get_test_scores('XGB test', xgb_test_preds, y_test)\n",
    "\n",
    "# Append to the results table\n",
    "results = pd.concat([results,xgb_test_scores], axis=0)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8L_LyIbgV1I"
   },
   "source": [
    "On the held‑out test set, recall matches the validation result, while precision declines somewhat, leading to small drops in F1 and accuracy. This gap is within a reasonable range for generalization error and supports using the validation‑selected XGBoost model as a stable performance estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5GNoz_QgWug"
   },
   "source": [
    "### **Confusion matrix**\n",
    "\n",
    "A confusion matrix summarizes the champion model’s churn predictions on the test set and highlights the trade‑off between correctly identifying churners and misclassifying retained users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WF3KErX8gXPc"
   },
   "outputs": [],
   "source": [
    "# Generate array of values for confusion matrix\n",
    "cm = confusion_matrix(y_test, xgb_test_preds, labels=xgb_cv.classes_)\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels=['retained','churned'])\n",
    "disp.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xL4OujkgYC3"
   },
   "source": [
    "The confusion matrix shows roughly three times as many false negatives as false positives, and the model correctly identifies about 16–17% of actual churners. This confirms that, even with improved recall over logistic regression, many at‑risk users remain undetected at the default decision threshold.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P33INGPmgY1o"
   },
   "source": [
    "### **Feature importance**\n",
    "\n",
    "Feature importance from the XGBoost model indicates which predictors contribute most strongly to churn predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N4fc2i8XgZoE"
   },
   "outputs": [],
   "source": [
    "plot_importance(xgb_cv.best_estimator_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EU3GIZNrga5z"
   },
   "source": [
    "The XGBoost model distributes importance across a broader set of predictors than the earlier logistic regression, which relied heavily on `activity_days`. Engineered features account for a majority of the top‑ranked predictors, underscoring how feature engineering can substantially improve model performance.\n",
    "\n",
    "Differences in feature importance between models reflect complex interactions among predictors: a feature that appears weak in one model may still carry signal in combination with others in a more flexible algorithm. This reinforces the value of testing multiple model families before discarding features as uninformative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ill21hQ4ej9-"
   },
   "source": [
    "## **Model insights and recommendations**\n",
    "\n",
    "Across all models, XGBoost achieved the highest recall on validation and test data, followed by random forest, with logistic regression performing worst on recall but best on interpretability. This pattern is consistent with expectations: tree‑based ensembles typically capture more complex patterns at the expense of transparency, while generalized linear models provide clearer explanations with more limited flexibility.\n",
    "\n",
    "- **Suitability for deployment:**  \n",
    "  The tuned XGBoost model improves recall relative to logistic regression and random forest but still misses many churners and generates a notable number of false positives. It is more suitable as a decision‑support tool and experimentation baseline than as a standalone system for high‑stakes, automated retention decisions.\n",
    "\n",
    "- **Train/validation/test split trade‑off:**  \n",
    "  Using separate validation and test sets reduces the data available for training but enables unbiased model selection on validation data and a cleaner final performance estimate on the untouched test set. This structure improves confidence that the selected model will generalize to new users.\n",
    "\n",
    "- **Logistic regression vs. tree‑based ensembles:**  \n",
    "  Logistic regression offers straightforward interpretability via coefficients and clear directionality of effects, making it valuable for explaining drivers of churn. Tree‑based ensembles like random forest and XGBoost typically deliver stronger predictive performance, handle nonlinearities and interactions, and require fewer distributional assumptions, at the cost of interpretability.\n",
    "\n",
    "- **Paths to improvement:**  \n",
    "  Further gains are likely to come from richer feature engineering (for example, temporal trends, volatility of behavior, or route diversity), targeted class‑weighting or threshold tuning to prioritize recall, and incorporating additional data on drive‑level behavior and in‑app interactions.\n",
    "\n",
    "- **Additional data needs:**  \n",
    "  Drive‑level details (timing, route type, context), more granular app‑interaction logs (searches, reports, confirmations), and geographic or contextual features would help distinguish structural churn from temporary disengagement and support more precise intervention strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1DHsmIEwaXUmfVT4tFzyOwyyfXAX0v6IF",
     "timestamp": 1675262571681
    },
    {
     "file_id": "1oNheYh5WbljxkvoK_BMkQTey2DWnFXMs",
     "timestamp": 1674856595373
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
