{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtNBZFHO3M7n"
   },
   "source": [
    "# **Waze User Churn: Logistic Regression Modeling**\n",
    "**This notebook develops and evaluates a binomial logistic regression model to predict user chrun for Waze based on app usage and driver behavior variables. The focus is on translating exploratory data analysis into a predictive model and interpreting model performance and feature effects in a business context.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgSbVJvomcVa"
   },
   "source": [
    "The analysis proceeds in three stages:\n",
    "- Exploratory data analysis (EDA) and assumption checks for logistic regression\n",
    "- Model building and evaluation using a binomial logistic regression classifier\n",
    "- Interpretation of model results and implications for churn-focused business decisions\n",
    "\n",
    "The objective is to build a model that predicts whether a user churns and to understand which behavioral features are most associated with churn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4L5F-G_cfLWL"
   },
   "source": [
    "### **Data and Libraries**\n",
    "The Waze churn dataset is loaded into a pandas DataFrame, and standard Python and scikit-learn tools are used for visualizations, feature engineering and logistics regression modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccfeg6X6eOVZ"
   },
   "outputs": [],
   "source": [
    "# Core libraries for data manipulation and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# tools for preprocessing, model training, and evaluation\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, \\\n",
    "recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjljvyG32kqe"
   },
   "source": [
    "The churn dataset is read from CSV into a DataFrame for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TyR3sBUYJBO8"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('waze_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnrvCSfHUWPv"
   },
   "source": [
    "## **Exploratory data analysis**\n",
    "\n",
    "EDA is used to understand class balance, identify missing values, and detect potential outliers or data quality issues that may influence a logistic regression model. Visual inspection of distributions and summary staistics support later decisions on feature engineer, outlier handling, and model assumptions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIcDG2e66wt9"
   },
   "source": [
    "### **Data structure, missing values, and outliers**\n",
    "\n",
    "The intial EDA examines dataset dimnesions, data types, missing labels, and summary statistics to understand overall structure and detect variables with extreme values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4Ag-sZhWg6K"
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the unique identifier not needed for modeling\n",
    "df = df.drop('ID',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check class balance of the churn label\n",
    "df['label'].value_counts(normalize='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary staistics for numeric variables\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ut0mWpGG6mkh"
   },
   "source": [
    "The dataset contains 700 missing values in the `label` (target) column, representing less than 5% of observations. \n",
    "\n",
    "Several usage-related variables(`sessions`,`drives`,`total_sessions`,`total_navigations_fav1`,`total_navigations_fav2`,`driven_km_drives`,`duration_minutes_drives`) exhibit extreme values, with maxima standard deviations above the upper quartile, indicating potential outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hl8G_0FR6Rvk"
   },
   "source": [
    "### **Feature Engineering**\n",
    "\n",
    "To better capture driving intensity, a new feature `km_per_driving_day` is created as the average distance driven per day for each user. Thsi condenses multiple raw variables into a single measure of driving behavior over the last month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCEzE-gwL5gq"
   },
   "outputs": [],
   "source": [
    "# mean distance driven per driving day\n",
    "df['km_per_driving_day'] = df['driven_km_drives']/df['driving_days']\n",
    "df['km_per_driving_day'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FetTHatPoR6n"
   },
   "outputs": [],
   "source": [
    "# replace infinite values with zero\n",
    "df.loc[df['km_per_driving_day']==np.inf, 'km_per_driving_day'] = 0\n",
    "df['km_per_driving_day'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ky5h_Aum3RK1"
   },
   "source": [
    "A binary `professional_driver` flag is introduced to distinguish heavy users (at least 60 drives and 15 or more driving days in the last month) from other drivers, using domain-informed thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huWmzNp2Xj8o"
   },
   "outputs": [],
   "source": [
    "# `professional_driver` column\n",
    "df['professional_driver'] = np.where((df['drives']>=60) & (df['driving_days']>=15), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jCAvucIVa3jE"
   },
   "outputs": [],
   "source": [
    "print(df['professional_driver'].value_counts())\n",
    "df.groupby(['professional_driver'])['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7kHbF4m-ZXA"
   },
   "source": [
    "Professional drivers show a churn rate of about 7.6%, compared with roughly 19.9% for non-professional users, indicating that high-activity drivers are substantially more like to be retained and that this feature may add predictive signal to the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgPul2DiY6T4"
   },
   "source": [
    "## **Model construction strategy**\n",
    "\n",
    "Predictor selection us guided by the business objective (predicting churn) and prior EDA, with multicollinearity used to drop redundant variables while retaining features with stronger relationships to churn. Iterative model runs and performance metrics such as accuracy, precision and recall help refine the feature set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07UJJm41ajgf"
   },
   "source": [
    "### **Handling missing labels and outliers**\n",
    "The `label` column is inspected for type and missingness, and rows with missing labels are dropped because they are relatively few and appear randomly distributed. Extreme values in several high-variance usage variables are winsorized at the 95th percentile to reduce the influece of outliers while retaining all observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tHFNCNj3ob30"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B34fDk52o2Uk"
   },
   "outputs": [],
   "source": [
    "# Drop rows with missing data in `label` column\n",
    "df = df.dropna(subset=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fRaU2JKpyXg"
   },
   "outputs": [],
   "source": [
    "# Impute outliers\n",
    "for column in ['sessions', 'drives', 'total_sessions', 'total_navigations_fav1', 'total_navigations_fav2', \n",
    "               'driven_km_drives', 'duration_minutes_drives']:\n",
    "    threshold = df[column].quantile(0.95)\n",
    "    df.loc[df[column] > threshold,column]=threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aU13ZoCMAN_s"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9dLJfBHAxNy"
   },
   "source": [
    "#### **Encoding the churn label**\n",
    "A binary target variable `label` is created where `1` indicates a churned user and `0` indicates a retained user, preserving the original categorical `label` for reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvcP3FxpAyws"
   },
   "outputs": [],
   "source": [
    "# Create binary `label2` column\n",
    "df['label2'] = np.where(df['label']=='churned', 1, 0)\n",
    "df[['label', 'label2']].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMj6QkK1cLmS"
   },
   "source": [
    "### **Logistic regression assumptinos**\n",
    "\n",
    "The logistic regression model assumes independent observations, a binary outcome, limited extreme outliers, low multicollinearity among predictors, and an approximately linear relationship between continuous predictors and the log-odds of churn. Independence is assumed from the data collection process, outliers have been mitigated by winsorization, and multicollinearity is assessed via the correlation matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SClNm5hWotj6"
   },
   "outputs": [],
   "source": [
    "# Generate a correlation matrix\n",
    "df.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5HVnvWmXrOCO"
   },
   "outputs": [],
   "source": [
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(df.corr(method='pearson'), vmin=-1, vmax=1, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation heatmap indicates many low correlated variables', fontsize=18)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wvq6jl6BqBX"
   },
   "source": [
    "The correlation matrix and heatmap highlight strong multicollinearity between `sessions` and `drives` (correlation near 1.0), and between `activity_days` and `driving_days` (correlation around 0.95).\n",
    "\n",
    "To reduce redundancy, only one variable from each highly correlated pair is retained in the final feature set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3ArC_5xa7Oi"
   },
   "source": [
    "### **Encoding device type**\n",
    "\n",
    "The `device` variable is binary-encoded as `device2` (0 for Android, ` for iPhone) so it can be used directly as a numeric predictor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QvDpwcQm0f35"
   },
   "outputs": [],
   "source": [
    "# Create new `device2` variable\n",
    "df['device2'] = np.where(df['device']=='Android', 0, 1)\n",
    "df[['device','device2']].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDYyjWssbnBG"
   },
   "source": [
    "### **Feature set and target**\n",
    "The feature matrix `x` excludes the original label fields, the unencoded device column, and two highly collinear variables (`session`, `driving_days`) in favor of `drives` and `activity_days`, which show slightly stronger associations with churn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzcDgLRET4d7"
   },
   "outputs": [],
   "source": [
    "# Isolate predictor variables\n",
    "X = df.drop(columns=['label','label2','device','sessions','driving_days'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0QFCHIJC3-I"
   },
   "outputs": [],
   "source": [
    "# Isolate target variable\n",
    "y=df['label2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOewKY740Beq"
   },
   "source": [
    "#### **Train-test split**\n",
    "\n",
    "The data is split into training and test sets using stratified sampling on the target to preserve the original churn vs. retention ratio, which helps obtain reliable performance estimates on an imbalanced classification probem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ulDZdfSS0FyD"
   },
   "outputs": [],
   "source": [
    "# Perform the train-test split, stratify=y preserves class proportions\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMUo8Ri-zK7r"
   },
   "source": [
    "A binomial logistic regression model without regularization (`penalty='none'`) is fitted to the training data to estimate the relationship between predictors and the log-odds of churn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrTNaDVZheyp"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty=None, max_iter=400)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPoDllWB6ULV"
   },
   "source": [
    "### **Model coefficients and intercept**\n",
    "\n",
    "Model coefficients quantify how each predictor is associated with the log‑odds of churn, holding other variables constant. Positive coefficients increase the log‑odds (and thus the probability) of churn, while negative coefficients decrease it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Ri-OHrlmd8j"
   },
   "outputs": [],
   "source": [
    "# coefficients with respective feature names\n",
    "pd.Series(model.coef_[0], index=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWeruvy1wksj"
   },
   "outputs": [],
   "source": [
    "# intercept value\n",
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Larger magnitude coefficients indicate features with stronger influence on the predicted log‑odds of churn, although statistical significance is not directly assessed in this scikit‑learn implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdVIjKNHmlY_"
   },
   "source": [
    "#### **Logit linearity check**\n",
    "\n",
    "To assess the assumption of approximate linearity between continuous predictors and the log‑odds of churn, predicted probabilities on the training data are transformed to logits and plotted against a key predictor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aY71xhehmo9i"
   },
   "outputs": [],
   "source": [
    "# Get the predicted probabilities of the training data\n",
    "training_probabilities = model.predict_proba(X_train)\n",
    "training_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tXq8gYnEw6M"
   },
   "outputs": [],
   "source": [
    "# Copy the training predictors and add the logit of chrun probability\n",
    "logit_data = X_train.copy()\n",
    "logit_data['logit'] = [np.log(prob[1]/prob[0]) for prob in training_probabilities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ix8VT0VEzQK"
   },
   "outputs": [],
   "source": [
    "# Plot regplot of `activity_days` vs log-odds\n",
    "sns.regplot(x='activity_days', y='logit', data=logit_data, scatter_kws={'s': 2, 'alpha': 0.5})\n",
    "plt.title('Log-odds: activity_days');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regplot for `activity_days` suggests an approximately monotonic, near‑linear relationship between activity and the log‑odds of churn, which is reasonably consistent with the logistic regression linearity assumption for this predictor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_l3bkxQdJ3a"
   },
   "source": [
    "## **Model evaluation**\n",
    "\n",
    "### **Classification performance**\n",
    "\n",
    "Predictions are generated on the held‑out test set, and standard classification metrics are used to assess how well the model identifies churned users versus retained users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSl5gbXfBPBN"
   },
   "outputs": [],
   "source": [
    "# Generate predictions on X_test\n",
    "y_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fU3v-XO49qm8"
   },
   "outputs": [],
   "source": [
    "# accuracy on the test data\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwRmSDS3eyeH"
   },
   "source": [
    "Accuracy provides an overall proportion of correct predictions but can be misleading when classes are imbalanced, so additional metrics are examined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBFU_dicBjwQ"
   },
   "outputs": [],
   "source": [
    "# confusion matrix display\n",
    "cm = confusion_matrix(y_test, y_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                             display_labels=['retained','churned'])\n",
    "disp.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39RM-g6UtbJ_"
   },
   "outputs": [],
   "source": [
    "# Calculate precision and recall manually\n",
    "precision = cm[1, 1] / (cm[0, 1] + cm[1, 1])\n",
    "recall = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTxfglLMGlr_"
   },
   "outputs": [],
   "source": [
    "# Full classification report\n",
    "target_labels = ['retained', 'churned']\n",
    "print(classification_report(y_test,y_preds, target_names=target_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8FQnyhnHFT7"
   },
   "source": [
    "The model achieves decent precision but relatively low recall for the churn class, indicating that it misses a substantial number of true churners (false negatives). For churn mitigation, this means many at‑risk users would not be flagged by the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSpkqurtHJSE"
   },
   "source": [
    "### **Feature importance (coefficients)**\n",
    "\n",
    "To visualize which features most strongly influence the model’s predictions, standardized coefficients are sorted and plotted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuT0aP6FHL6B"
   },
   "outputs": [],
   "source": [
    "# Create a list of (feature, coefficient) tuples\n",
    "feature_importance = list(zip(X_train.columns, model.coef_[0]))\n",
    "\n",
    "feature_importance = sorted(feature_importance, key=lambda x:x[1], reverse = True)\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kaam2OD8HOP5"
   },
   "outputs": [],
   "source": [
    "# Plot the feature importances\n",
    "sns.barplot(x = [x[1] for x in feature_importance],\n",
    "           y = [x[0] for x in feature_importance],\n",
    "           orient = 'h')\n",
    "plt.title('Feature importance');\n",
    "plt.xlabel('Coefficient');\n",
    "plt.ylabel('Feature');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features with larger positive coefficients are associated with higher churn risk, while those with large negative coefficients are associated with retention, holding other variables constant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6AlDDyhdzmG"
   },
   "source": [
    "## **Model insights and business implications**\n",
    "\n",
    "- `activity_days` is by far the most influential variable in the model’s prediction, with higher activity strongly associated with retention and lower churn probability. This aligns with earlier EDA showing that more active users tend to stay on the platform.  \n",
    "- In prior EDA, churn increased as `km_per_driving_day` rose, and the correlation heatmap indicated a strong positive association with churn. In the multivariate logistic model, however, this feature becomes relatively weak, suggesting that its apparent effect is largely explained by other, more informative usage variables.\n",
    "\n",
    "In a multiple logistic regression model, predictors can interact and share variance, which can make some features look less important once others are included. This can improve predictive performance while making interpretation less intuitive.\n",
    "\n",
    "From a business perspective, the current model’s low recall on churners limits its usefulness for high‑stakes retention campaigns, where missing at‑risk users is costly. It is more suitable as a baseline model to guide further feature engineering and model experimentation rather than as a deployment‑ready churn predictor.\n",
    "\n",
    "Potential improvements include:\n",
    "- Engineering additional behavioral and temporal features (for example, recent changes in usage, patterns of cancellation, or route diversity) to capture early signs of disengagement.  \n",
    "- Exploring alternative model specifications (feature subsets, regularization, and class‑weighting) and comparing against more flexible machine learning models in the subsequent notebook.\n",
    "\n",
    "Additional data such as drive‑level details (route characteristics, duration, time of day) and richer in‑app interaction signals (reports, confirmations, search behavior) would likely improve both predictive power and actionability for churn prevention strategies.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "10VdUGxtn9_2OaVJAptrlOkngf4ZKm52V",
     "timestamp": 1671215458572
    },
    {
     "file_id": "1Et0HEKWEY0dZ0BaNZeH189bi-PnC-iUH",
     "timestamp": 1671209438879
    },
    {
     "file_id": "16ynSVRiYFz40jV9BFaXgY62vWJWEVjFA",
     "timestamp": 1671052218770
    },
    {
     "file_id": "15PPfvGb4OuUkuQoTTpKKgSeY9o9a_XrL",
     "timestamp": 1669649625868
    },
    {
     "file_id": "1_uBujapIzHItho27E-iPg9wn3aHtsjLG",
     "timestamp": 1664565386285
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
